{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HJ4tFmP4JFL"
      },
      "source": [
        "# Prepare audio data for image recognition\n",
        "\n",
        "The data is pretty good, but there's a few samples that aren't exactly 1 second long and some samples that are either truncated or don't contain very much of the word.\n",
        "\n",
        "The code in the notebook attempts to filter out the broken audio so that we are only using good audio.\n",
        "\n",
        "We then generate spectrograms of each word. We mix in background noise with the words to make it a more realistic audio sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rvfrcR1b_jUN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-13 20:36:14--  http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.163.48, 2404:6800:4012:4::2010\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.163.48|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428923189 (2.3G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.02.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   2.26G  51.0MB/s    in 46s     \n",
            "\n",
            "2022-07-13 20:37:01 (50.2 MB/s) - ‘speech_commands_v0.02.tar.gz’ saved [2428923189/2428923189]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "!mkdir speech_data\n",
        "!tar -xzf speech_commands_v0.02.tar.gz -C speech_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QeHoru3B4JFO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.io import gfile\n",
        "import tensorflow_io as tfio\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.ops import gen_audio_ops as audio_ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uHHfDOd_4JFQ"
      },
      "outputs": [],
      "source": [
        "SPEECH_DATA='speech_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CW0xZmgu4JFQ"
      },
      "outputs": [],
      "source": [
        "# The audio is all sampled at 16KHz and should all be 1 second in length - so 1 second is 16000 samples\n",
        "EXPECTED_SAMPLES=16000\n",
        "# Noise floor to detect if any audio is present\n",
        "NOISE_FLOOR=0.1\n",
        "# How many samples should be abover the noise floor?\n",
        "MINIMUM_VOICE_LENGTH=EXPECTED_SAMPLES/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gXHIzBGZ4JFQ"
      },
      "outputs": [],
      "source": [
        "# list of folders we want to process in the speech_data folder\n",
        "command_words = [\n",
        "    'follow',\n",
        "    'stop',\n",
        "    'go',\n",
        "    '_invalid',\n",
        "]\n",
        "nonsense_words = [\n",
        "    'up',\n",
        "    'down',\n",
        "    'forward',\n",
        "    'backward',\n",
        "    'left',\n",
        "    'right',\n",
        "    'on',\n",
        "    'off',\n",
        "    'learn',\n",
        "    'yes',\n",
        "    'no',\n",
        "    'zero',\n",
        "    'one',\n",
        "    'two',\n",
        "    'three',\n",
        "    'four',\n",
        "    'five',\n",
        "    'six',\n",
        "    'seven',\n",
        "    'eight',\n",
        "    'nine',\n",
        "    'tree',\n",
        "    'bed',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'dog',\n",
        "    'happy',\n",
        "    'house',\n",
        "    'marvin',\n",
        "    'sheila',\n",
        "    'visual',\n",
        "    'wow',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sPVh0v6S4JFR"
      },
      "outputs": [],
      "source": [
        "# get all the files in a directory\n",
        "def get_files(word):\n",
        "    return gfile.glob(SPEECH_DATA + '/'+word+'/*.wav')\n",
        "\n",
        "# get the location of the voice\n",
        "def get_voice_position(audio, noise_floor):\n",
        "    audio = audio - np.mean(audio)\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    return tfio.audio.trim(audio, axis=0, epsilon=noise_floor)\n",
        "\n",
        "# Work out how much of the audio file is actually voice\n",
        "def get_voice_length(audio, noise_floor):\n",
        "    position = get_voice_position(audio, noise_floor)\n",
        "    return (position[1] - position[0]).numpy()\n",
        "\n",
        "# is enough voice present?\n",
        "def is_voice_present(audio, noise_floor, required_length):\n",
        "    voice_length = get_voice_length(audio, noise_floor)\n",
        "    return voice_length >= required_length\n",
        "\n",
        "# is the audio the correct length?\n",
        "def is_correct_length(audio, expected_length):\n",
        "    return (audio.shape[0]==expected_length).numpy()\n",
        "\n",
        "\n",
        "def is_valid_file(file_name):\n",
        "    # load the audio file\n",
        "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
        "    # check the file is long enough\n",
        "    if not is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n",
        "        return False\n",
        "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
        "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
        "    audio = audio - np.mean(audio)\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    # is there any voice in the audio?\n",
        "    if not is_voice_present(audio, NOISE_FLOOR, MINIMUM_VOICE_LENGTH):\n",
        "        return False\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BfPQz_5U4JFS"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(audio):\n",
        "    # normalise the audio\n",
        "    audio = audio - np.mean(audio)\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    # create the spectrogram\n",
        "    spectrogram = audio_ops.audio_spectrogram(audio,\n",
        "                                              window_size=320,\n",
        "                                              stride=160,\n",
        "                                              magnitude_squared=True).numpy()\n",
        "    # reduce the number of frequency bins in our spectrogram to a more sensible level\n",
        "    spectrogram = tf.nn.pool(\n",
        "        input=tf.expand_dims(spectrogram, -1),\n",
        "        window_shape=[1, 6],\n",
        "        strides=[1, 6],\n",
        "        pooling_type='AVG',\n",
        "        padding='SAME')\n",
        "    spectrogram = tf.squeeze(spectrogram, axis=0)\n",
        "    spectrogram = np.log10(spectrogram + 1e-6)\n",
        "    return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AJNRg_ih4JFS"
      },
      "outputs": [],
      "source": [
        "# process a file into its spectrogram\n",
        "def process_file(file_path):\n",
        "    # load the audio file\n",
        "    audio_tensor = tfio.audio.AudioIOTensor(file_path)\n",
        "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
        "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
        "    audio = audio - np.mean(audio)\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    # randomly reposition the audio in the sample\n",
        "    voice_start, voice_end = get_voice_position(audio, NOISE_FLOOR)\n",
        "    end_gap=len(audio) - voice_end\n",
        "    random_offset = np.random.uniform(0, voice_start+end_gap)\n",
        "    audio = np.roll(audio,-random_offset+end_gap)\n",
        "    # add some random background noise\n",
        "    background_volume = np.random.uniform(0, 0.1)\n",
        "    # get the background noise files\n",
        "    background_files = get_files('_background_noise_')\n",
        "    background_file = np.random.choice(background_files)\n",
        "    background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
        "    background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
        "    # normalise the background noise\n",
        "    background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
        "    background = background - np.mean(background)\n",
        "    background = background / np.max(np.abs(background))\n",
        "    # mix the audio with the scaled background\n",
        "    audio = audio + background_volume * background\n",
        "    # get the spectrogram\n",
        "    return get_spectrogram(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ec5JYgqS4JFT"
      },
      "outputs": [],
      "source": [
        "train = []\n",
        "validate = []\n",
        "test = []\n",
        "\n",
        "TRAIN_SIZE=0.8\n",
        "VALIDATION_SIZE=0.1\n",
        "TEST_SIZE=0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XHszhHq_4JFU"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d61215b8c1f544829b6bb14cfe2c6c97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing words:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c697a499582543d0b9e01432dee8971e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Checking:   0%|          | 0/1579 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-07-13 23:06:01.911083: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
            "2022-07-13 23:06:02.106511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-07-13 23:06:02.106875: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.106946: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.106982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.107019: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.107064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.107103: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.107139: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.107174: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2022-07-13 23:06:02.107179: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-07-13 23:06:02.108486: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcf63c4cb6f14ed9b6e3d7085e214937",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "follow (0):   0%|          | 0/42520 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2888342/2884054158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mrepeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'follow'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mprocess_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# all the nonsense words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_2888342/2884054158.py\u001b[0m in \u001b[0;36mprocess_word\u001b[0;34m(word, label, repeat)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# get the training samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     train.extend(\n\u001b[0;32m---> 17\u001b[0;31m         process_files(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_2888342/2884054158.py\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(file_names, label, repeat)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{word} ({label})\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# process the files for a word into the spectrogram and one hot encoding word value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_2888342/2884054158.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{word} ({label})\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# process the files for a word into the spectrogram and one hot encoding word value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_2888342/1746511292.py\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbackground_volume\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# get the spectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_2888342/2521469329.py\u001b[0m in \u001b[0;36mget_spectrogram\u001b[0;34m(audio)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# normalise the audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# create the spectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1398\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtraceback_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m         \u001b[0;31m# force_same_dtype=False to preserve existing TF behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6693\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6694\u001b[0m   \"\"\"\n\u001b[0;32m-> 6695\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6696\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minternal_name_scope_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_execution_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2219\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;34m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mones_rank_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def process_files(file_names, label, repeat=1):\n",
        "    file_names = tf.repeat(file_names, repeat).numpy()\n",
        "    return [(process_file(file_name), label) for file_name in tqdm(file_names, desc=f\"{word} ({label})\", leave=False)]\n",
        "\n",
        "# process the files for a word into the spectrogram and one hot encoding word value\n",
        "def process_word(word, label, repeat=1):\n",
        "    # get a list of files names for the word\n",
        "    file_names = [file_name for file_name in tqdm(get_files(word), desc=\"Checking\", leave=False) if is_valid_file(file_name)]\n",
        "    # randomly shuffle the filenames\n",
        "    np.random.shuffle(file_names)\n",
        "    # split the files into train, validate and test buckets\n",
        "    train_size=int(TRAIN_SIZE*len(file_names))\n",
        "    validation_size=int(VALIDATION_SIZE*len(file_names))\n",
        "    test_size=int(TEST_SIZE*len(file_names))\n",
        "    # get the training samples\n",
        "    train.extend(\n",
        "        process_files(\n",
        "            file_names[:train_size],\n",
        "            label,\n",
        "            repeat=repeat\n",
        "        )\n",
        "    )\n",
        "    # and the validation samples\n",
        "    validate.extend(\n",
        "        process_files(\n",
        "            file_names[train_size:train_size+validation_size],\n",
        "            label,\n",
        "            repeat=repeat\n",
        "        )\n",
        "    )\n",
        "    # and the test samples\n",
        "    test.extend(\n",
        "        process_files(\n",
        "            file_names[train_size+validation_size:],\n",
        "            label,\n",
        "            repeat=repeat\n",
        "        )\n",
        "    )\n",
        "\n",
        "# process all the command words\n",
        "for word in tqdm(command_words, desc=\"Processing words\"):\n",
        "    if '_' not in word:\n",
        "        repeat = 30 if word in ('follow') else 15\n",
        "        process_word(word, command_words.index(word), repeat=repeat)\n",
        "        \n",
        "# all the nonsense words\n",
        "for word in tqdm(nonsense_words, desc=\"Processing words\"):\n",
        "    if '_' not in word:\n",
        "        process_word(word, command_words.index('_invalid'), repeat=1)\n",
        "\n",
        "print(len(train), len(test), len(validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqlJTw6f4JFW"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dbcb9e8fd3346a48d8dfc82d023af9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Background Noise:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f0cf2cb8ac64555b33281a4969f3969",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "speech_data/_background_noise_/white_noise.wav:   0%|          | 0/59 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc8e8ed15c0446b2908e92b13a8b1b03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Simulated Words:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecaf752917b84aedad4307262ddb8218",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "speech_data/_background_noise_/doing_the_dishes.wav:   0%|          | 0/95 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4e3a27d59b0482da8317bd0e0a30bda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Simulated Words:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93c1675825264a8cbb37b22663d0aef2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "speech_data/_background_noise_/dude_miaowing.wav:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9f0a292c18041c6a10b2077478a02d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Simulated Words:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ed41ea3133e46cebe92ec92147295f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "speech_data/_background_noise_/exercise_bike.wav:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c329072a426542578ccfe0a710e8d6b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Simulated Words:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41da1b1052e34a20b646c1de9dc66c3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "speech_data/_background_noise_/running_tap.wav:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbb232a5491046108caf09c86eead2ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Simulated Words:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b453f5b61ec74fb1b7b0c7d46c175dab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "speech_data/_background_noise_/pink_noise.wav:   0%|          | 0/59 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34b616f32fdf4ba19879a2f49c3278b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Simulated Words:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "292318 36729 36440\n"
          ]
        }
      ],
      "source": [
        "# process the background noise files\n",
        "def process_background(file_name, label):\n",
        "    # load the audio file\n",
        "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
        "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
        "    audio_length = len(audio)\n",
        "    samples = []\n",
        "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 16000), desc=file_name, leave=False):\n",
        "        section_end = section_start + EXPECTED_SAMPLES\n",
        "        section = audio[section_start:section_end]\n",
        "        # get the spectrogram\n",
        "        spectrogram = get_spectrogram(section)\n",
        "        samples.append((spectrogram, label))\n",
        "\n",
        "    # simulate random utterances\n",
        "    for section_index in tqdm(range(1000), desc=\"Simulated Words\", leave=False):\n",
        "        section_start = np.random.randint(0, audio_length - EXPECTED_SAMPLES)\n",
        "        section_end = section_start + EXPECTED_SAMPLES\n",
        "        section = np.reshape(audio[section_start:section_end], (EXPECTED_SAMPLES))\n",
        "\n",
        "        result = np.zeros((EXPECTED_SAMPLES))\n",
        "        # create a pseudo bit of voice\n",
        "        voice_length = np.random.randint(MINIMUM_VOICE_LENGTH/2, EXPECTED_SAMPLES)\n",
        "        voice_start = np.random.randint(0, EXPECTED_SAMPLES - voice_length)\n",
        "        hamming = np.hamming(voice_length)\n",
        "        # amplify the voice section\n",
        "        result[voice_start:voice_start+voice_length] = hamming * section[voice_start:voice_start+voice_length]\n",
        "        # get the spectrogram\n",
        "        spectrogram = get_spectrogram(np.reshape(section, (16000, 1)))\n",
        "        samples.append((spectrogram, label))\n",
        "        \n",
        "    \n",
        "    np.random.shuffle(samples)\n",
        "    \n",
        "    train_size=int(TRAIN_SIZE*len(samples))\n",
        "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
        "    test_size=int(TEST_SIZE*len(samples))\n",
        "    \n",
        "    train.extend(samples[:train_size])\n",
        "\n",
        "    validate.extend(samples[train_size:train_size+validation_size])\n",
        "\n",
        "    test.extend(samples[train_size+validation_size:])\n",
        "\n",
        "        \n",
        "for file_name in tqdm(get_files('_background_noise_'), desc=\"Processing Background Noise\"):\n",
        "    process_background(file_name, command_words.index(\"_invalid\"))\n",
        "    \n",
        "print(len(train), len(test), len(validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl5Blrod4JFW"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0eb3927529854f40b64d2f6751373c50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing problem noise: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def process_problem_noise(file_name, label):\n",
        "    samples = []\n",
        "    # load the audio file\n",
        "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
        "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
        "    audio_length = len(audio)\n",
        "    samples = []\n",
        "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 800), desc=file_name, leave=False):\n",
        "        section_end = section_start + EXPECTED_SAMPLES\n",
        "        section = audio[section_start:section_end]\n",
        "        # get the spectrogram\n",
        "        spectrogram = get_spectrogram(section)\n",
        "        samples.append((spectrogram, label))\n",
        "        \n",
        "    np.random.shuffle(samples)\n",
        "    \n",
        "    train_size=int(TRAIN_SIZE*len(samples))\n",
        "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
        "    test_size=int(TEST_SIZE*len(samples))\n",
        "    \n",
        "    train.extend(samples[:train_size])\n",
        "    validate.extend(samples[train_size:train_size+validation_size])\n",
        "    test.extend(samples[train_size+validation_size:])\n",
        "\n",
        "\n",
        "for file_name in tqdm(get_files(\"_problem_noise_\"), desc=\"Processing problem noise\"):\n",
        "    process_problem_noise(file_name, command_words.index(\"_invalid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUP16tsf4JFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "292318 36729 36440\n"
          ]
        }
      ],
      "source": [
        "print(len(train), len(test), len(validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4pIiG1E4JFX"
      },
      "outputs": [],
      "source": [
        "# randomise the training samples\n",
        "np.random.shuffle(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xCDpfkx4JFX"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = zip(*train)\n",
        "X_validate, Y_validate = zip(*validate)\n",
        "X_test, Y_test = zip(*test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TgJb6lq4JFY"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2888342/2380948055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save the computed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m np.savez_compressed(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"training_spectrogram.npz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     X=X_train, Y=Y_train)\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved training data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# save the computed data\n",
        "np.savez_compressed(\n",
        "    \"training_spectrogram.npz\",\n",
        "    X=X_train, Y=Y_train)\n",
        "print(\"Saved training data\")\n",
        "np.savez_compressed(\n",
        "    \"validation_spectrogram.npz\",\n",
        "    X=X_validate, Y=Y_validate)\n",
        "print(\"Saved validation data\")\n",
        "np.savez_compressed(\n",
        "    \"test_spectrogram.npz\",\n",
        "    X=X_test, Y=Y_test)\n",
        "print(\"Saved test data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSS-QVyA4JFY"
      },
      "outputs": [],
      "source": [
        "# get the width and height of the spectrogram \"image\"\n",
        "IMG_WIDTH=X_train[0].shape[0]\n",
        "IMG_HEIGHT=X_train[0].shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t1h0MNT4JFZ"
      },
      "outputs": [],
      "source": [
        "def plot_images2(images_arr, imageWidth, imageHeight):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(10, 10))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip(images_arr, axes):\n",
        "        ax.imshow(np.reshape(img, (imageWidth, imageHeight)))\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMvmIqwP4JFZ"
      },
      "outputs": [],
      "source": [
        "word_index = command_words.index(\"follow\")\n",
        "\n",
        "X_left = np.array(X_train)[np.array(Y_train) == word_index]\n",
        "plot_images2(X_left[:10], IMG_WIDTH, IMG_HEIGHT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmWe77B94JFZ"
      },
      "outputs": [],
      "source": [
        "word_index = command_words.index(\"stop\")\n",
        "\n",
        "X_right = np.array(X_train)[np.array(Y_train) == word_index]\n",
        "plot_images2(X_right[:10], IMG_WIDTH, IMG_HEIGHT)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of process_command_traning_data.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
